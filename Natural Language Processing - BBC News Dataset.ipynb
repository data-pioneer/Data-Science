{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2899fddc",
   "metadata": {},
   "source": [
    "##  Problem Statement: Natural Language Processing on BBC News dataset available on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760024d",
   "metadata": {},
   "source": [
    "In this tutorial, I have performed Natural Language Processing tasks using Python libraries such as NLTK, SpaCy, Word2Vec, and TF-IDF. I have used various techniques such as tokenization, stemming, lemmatization, and document similarity calculation using these libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa71c8",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f377eff",
   "metadata": {},
   "source": [
    "##### 1. Import the necessary libraries: Start by importing the required libraries,including NLTK, SpaCy, gensim, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11bb147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: fst-pso in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4849f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d14d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from spacy import displacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial import distance\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7557e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jaspreetkaur/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11f2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaspreetkaur/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a85e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1e07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed3d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onjWNL = WordNetLemmatizer()\n",
    "objPorterStemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e53b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "stWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60aee6",
   "metadata": {},
   "source": [
    "###### 2. Load the dataset: Load the BBCNewsdataset(BBC_DATA.csv)into a pandas DataFrame using the read_csv() function. The dataset contains 2,225 rows and 2 columns, with the first column containing the text of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e671f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBbcNews = pd.read_csv(\"bbc_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692b161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTop5Rows = dataBbcNews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ade00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20536 entries, 0 to 20535\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        20536 non-null  object\n",
      " 1   pubDate      20536 non-null  object\n",
      " 2   guid         20536 non-null  object\n",
      " 3   link         20536 non-null  object\n",
      " 4   description  20536 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 802.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataBbcNews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3b204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Ukraine: Angry Zelensky vows to punish Russian...\n",
       "1        War in Ukraine: Taking cover in a town under a...\n",
       "2               Ukraine war 'catastrophic for global food'\n",
       "3        Manchester Arena bombing: Saffie Roussos's par...\n",
       "4        Ukraine conflict: Oil price soars to highest l...\n",
       "                               ...                        \n",
       "20531    UCI Cycling World Championships 2023: Tom Pidc...\n",
       "20532    Hibernian 3-1 Luzern: Aston Villa Europa Confe...\n",
       "20533    The Hundred 2023: Stevie Eskinazi steers Welsh...\n",
       "20534    The Hundred 2023: Shabnam Ismail's match-winni...\n",
       "20535              Mortgage rates: Five ways to save money\n",
       "Name: title, Length: 20536, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textColumn = dataBbcNews['title']\n",
    "textColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d6c06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        The Ukrainian president says the country will ...\n",
       "1        Jeremy Bowen was on the frontline in Irpin, as...\n",
       "2        One of the world's biggest fertiliser firms sa...\n",
       "3        The parents of the Manchester Arena bombing's ...\n",
       "4        Consumers are feeling the impact of higher ene...\n",
       "                               ...                        \n",
       "20531    Great Britain's Tom Pidcock is accused of \"cra...\n",
       "20532    Two late goals give Hibernian a potentially pr...\n",
       "20533    Stevie Eskinazi hits a classy 43 from 18 balls...\n",
       "20534    Watch the best plays from day 10 of The Hundre...\n",
       "20535    Experts give advice for those who might be wor...\n",
       "Name: description, Length: 20536, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleColumn =  dataBbcNews['description']\n",
    "articleColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbedf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a5e34f5",
   "metadata": {},
   "source": [
    "##### 3. Tokenization with NLTK:Implement tokenization using NLTK's word_tokenize() and sent_tokenize() functions. Apply these functions to a sample news article from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d23137",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d27651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Ukrainian',\n",
       " 'president',\n",
       " 'says',\n",
       " 'the',\n",
       " 'country',\n",
       " 'will',\n",
       " 'not',\n",
       " 'forgive',\n",
       " 'or',\n",
       " 'forget',\n",
       " 'those',\n",
       " 'who',\n",
       " 'murder',\n",
       " 'its',\n",
       " 'civilians',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(articleColumn[0])\n",
    "# perofrm word tokenize of 1 row of article column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e77470d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeremy Bowen was on the frontline in Irpin, as residents came under Russian fire while trying to flee.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(articleColumn[1]) \n",
    "# perofrm sentence tokenize of 2 row of article column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6978af5",
   "metadata": {},
   "source": [
    "##### 4. Stemming and Lemmatization with NLTK:Implement stemming and lemmatization using NLTK's PorterStemmer and WordNetLemmatizer functions. Apply these functions to a sample news article from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3410494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====implement stemming on a single row values===\n",
      "The actual word is ==> ukrainian and stem word is ==> ukrainian\n",
      "The actual word is ==> president and stem word is ==> presid\n",
      "The actual word is ==> says and stem word is ==> say\n",
      "The actual word is ==> country and stem word is ==> countri\n",
      "The actual word is ==> forgive and stem word is ==> forgiv\n",
      "The actual word is ==> forget and stem word is ==> forget\n",
      "The actual word is ==> murder and stem word is ==> murder\n",
      "The actual word is ==> civilians and stem word is ==> civilian\n",
      "The actual word is ==> . and stem word is ==> .\n"
     ]
    }
   ],
   "source": [
    "# implement stemming on a single row. \n",
    "sentence = articleColumn[0].lower()\n",
    "wordTokenized = word_tokenize(sentence)\n",
    "#wordTokenized\n",
    "sentenceCleaned = [s for s in wordTokenized if s not in stWords]\n",
    "# sentenceCleaned\n",
    "print(\"====implement stemming on a single row values===\")\n",
    "for i in sentenceCleaned:\n",
    "    print(f\"The actual word is ==> {i} and stem word is ==> {objPorterStemmer.stem(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f7a951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====implement Lemmatization on single row values===\n",
      "The actual word is ==> ukrainian and lemmatize word is ==> ukrainian\n",
      "The actual word is ==> president and lemmatize word is ==> president\n",
      "The actual word is ==> says and lemmatize word is ==> say\n",
      "The actual word is ==> country and lemmatize word is ==> country\n",
      "The actual word is ==> forgive and lemmatize word is ==> forgive\n",
      "The actual word is ==> forget and lemmatize word is ==> forget\n",
      "The actual word is ==> murder and lemmatize word is ==> murder\n",
      "The actual word is ==> civilians and lemmatize word is ==> civilian\n",
      "The actual word is ==> . and lemmatize word is ==> .\n"
     ]
    }
   ],
   "source": [
    "# implement Lemmatization on a single row.\n",
    "print(\"====implement Lemmatization on single row values===\")\n",
    "for i in sentenceCleaned:\n",
    "  print(f\"The actual word is ==> {i} and lemmatize word is ==> {onjWNL.lemmatize(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882e1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns, Pos tag value of words, which we use later in lemmatize method\n",
    "def getPosTag(tagValue):\n",
    "    if tagValue.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tagValue.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tagValue.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tagValue.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a52390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: ukrainian, Lemmatized word: ukrainian\n",
      "Original word: president, Lemmatized word: president\n",
      "Original word: says, Lemmatized word: say\n",
      "Original word: country, Lemmatized word: country\n",
      "Original word: forgive, Lemmatized word: forgive\n",
      "Original word: forget, Lemmatized word: forget\n",
      "Original word: murder, Lemmatized word: murder\n",
      "Original word: civilians, Lemmatized word: civilian\n"
     ]
    }
   ],
   "source": [
    "# lemmatized words according to their POS tag.... \n",
    "tokenized = sent_tokenize(sentence)\n",
    "for i in tokenized:  \n",
    "    wordsList = word_tokenize(i)\n",
    "\n",
    "    # remove stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stWords]\n",
    " \n",
    "    #  call method to get POS tag of word\n",
    "    PartsofSpeech = pos_tag(wordsList)\n",
    "    for lis in PartsofSpeech:\n",
    "        if getPosTag(lis[1]) != None:\n",
    "            word_lemma = onjWNL.lemmatize(lis[0], pos = getPosTag(lis[1]))\n",
    "            print(f\"Original word: {lis[0]}, Lemmatized word: {word_lemma}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18ed72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5bcc5fb",
   "metadata": {},
   "source": [
    "##### 5. Named Entity Recognition with SpaCy: Use SpaCy's pre-trained model to perform named entity recognition on a sample news article from the dataset. Visualize the named entities using displaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8432cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaspreetkaur/anaconda3/lib/python3.11/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    boris johnson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is to meet the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    canadian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    dutch\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " pms, as mps debate new laws targeting oligarchs.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# implement Named entity recognition on 5 row of article column\n",
    "sentence = articleColumn[5].lower() \n",
    "doc = nlp(sentence)\n",
    "NER = [(ent.text,ent.label_) for ent in doc.ents]\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264bc49c",
   "metadata": {},
   "source": [
    "##### 6. Word2Vec with gensim: Implement Word2Vec using gensim's Word2Vec function on the entire dataset.  Train the model and get the vector representation of a sample word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e71517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement word2vec on dataset description column... \n",
    "dataBbcNews['tokenized_sents'] = dataBbcNews.apply(lambda row: nltk.word_tokenize(row['description']), axis=1)\n",
    "#dataBbcNews['tokenized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edd4308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1485838, 2015590)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Word2Vec model object\n",
    "wordModel=Word2Vec(window=5, min_count=2, workers=4, sg=0)\n",
    "\n",
    "# Train word2vec model with specific parameters\n",
    "wordModel.build_vocab(dataBbcNews['tokenized_sents'],\n",
    "                  progress_per=1000)\n",
    "wordModel.train(dataBbcNews['tokenized_sents'], \n",
    "            total_examples=wordModel.corpus_count, epochs=wordModel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac92ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Note: i have commented some code for more clearance..\n",
    "###### And display only required code... which is required only for problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d35616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordModel.wv[\"president\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660fb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordModel.wv[\"civilians\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80762de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordModel.wv[\"Manchester\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e146fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7765721"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordModel.wv.similarity(w1=\"civilians\", w2=\"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbad43bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41614118"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordModel.wv.similarity(w1=\"civilians\", w2=\"Manchester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743cdbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9338075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordModel.wv.similarity(w1=\"parents\", w2=\"Ukrainian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7407f12",
   "metadata": {},
   "source": [
    "##### 7. TF-IDF with scikit-learn: Implement TF-IDF using scikit-learn's TfidfVectorizer function on the entire dataset.  Transform the dataset using the fitted vectorizer and calculate the cosine similarity between two news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b8132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert database description column to list, \n",
    "# as TfidfVectorizer fit_transform method takes list as arguments...\n",
    "descList = dataBbcNews['description'].tolist()\n",
    "descList = descList[1:5]\n",
    "#descList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d17cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1fe03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass list object to fit_transform method\n",
    "tfidfMatrix = vectorizer.fit_transform(descList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a1b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f46e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert matrix 1 row to list, after fetching from array...\n",
    "textToVector1 = tfidfMatrix.toarray()[0].tolist()\n",
    "#textToVector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36732359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert matrix 2 row to list, after fetching from array...\n",
    "textToVector2 = tfidfMatrix.toarray()[1].tolist()\n",
    "# textToVector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b049e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two sentences are equal to  7.64 %\n"
     ]
    }
   ],
   "source": [
    "# find cosine similarity between two rows in term of percentage....\n",
    "cosine = distance.cosine(textToVector1, textToVector2)\n",
    "print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "658b6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "textToVector3 = tfidfMatrix.toarray()[3].tolist()\n",
    "# textToVector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008236ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two sentences are equal to  5.82 %\n"
     ]
    }
   ],
   "source": [
    "# find cosine similarity between two rows in term of percentage....\n",
    "cosine = distance.cosine(textToVector1, textToVector3)\n",
    "print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
